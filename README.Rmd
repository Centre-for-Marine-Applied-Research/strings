---
output: github_document
editor_options: 
  chunk_output_type: console
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}

knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
  #fig.path = "man/figures/README-",
  #out.width = "100%"
)

library(badger)
library(dplyr)
library(kableExtra)
library(usethis)

repo <- "centre-for-marine-applied-research/strings"

```

# strings

<!-- badges: start -->

[![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0) `r badge_devel(repo, "blue")` `r badge_codefactor(repo)` `r badge_github_actions(repo)`

<!-- badges: end -->

Compile, format, and visualize water quality (temperature, dissolved oxygen, salinity) data measured by different brands of autonomous sensors. 

## Installation

You can install the development version of strings from [GitHub](https://github.com/) with:

``` r
# install.packages("devtools")
devtools::install_github("Centre-for-Marine-Applied-Research/strings")
```

## Background

The Centre for Marine Applied Research ([CMAR](https://cmar.ca/)) coordinates an extensive [Coastal Monitoring Program](https://cmar.ca/2020/07/14/coastal-monitoring-program/) that measures [Essential Ocean Variables](https://www.goosocean.org/index.php?option=com_content&view=article&id=14&Itemid=114) (e.g., temperature, dissolved oxygen, salinity, sea state, currents), typically within 1 km of the coast of Nova Scotia, Canada. The `strings` package is used to compile, format, and visualize water quality data collected through this program.


Water quality data (temperature, dissolved oxygen, and salinity) is collected using “sensor strings”. Each string is attached to the seafloor by an anchor and suspended by a sub-surface buoy, with autonomous sensors attached at various depths (Figure 1). A string typically includes sensors from three manufacturers: Hobo (Onset?), aquaMeasure (InnovaSea?), and Vemco (Table X). Strings are typically deployed at a station for 6 – 12 months and data are measured every 1 minute to 1 hour, resulting in tens- to hundreds- of thousands of observations for a single deployment. 


[](inst/image/README-fig1.PNG)

```{r, echo=FALSE, out.width="65%", fig.align='center'}

# knitr::include_graphics("inst/image/README_fig1.PNG")
#knitr::include_graphics("man/figures/README-fig1.PNG")

```

```{r, echo=FALSE, out.width="65%", fig.align='center'}

# knitr::include_graphics("inst/image/README_fig1.PNG")
knitr::include_graphics("README-fig1.PNG")

```


(After retrieval?) Data from each sensor is exported to a separate *csv file (using manufacturer-specific software). Each type of sensor generates a data file with unique columns and header fields, which poses a significant challenge for compiling all data from a deployment into a single format for analysis.

The strings package was originally built to address this challenge, and now offers functions to compile, format, visualize, and convert sensor string data.

`strings` was developed specifically to streamline CMAR’s workflow, but was designed to be flexible enough that other users can apply it to process data from the accepted sensors. Refer to vignettes for more detail.

Processed data from CMAR’s Coastal Monitoring Program can be viewed and downloaded from …. [cheat sheet].


include example of compiled data here?


## Example 

```{r example}
library(strings)

library(readr)
```

Consider a string deployed from May 31, 2019 to October 19, 2019 with three sensors:

```{r, echo=FALSE}

tibble(
  "Sensor" = c("HOBO Pro V2", "aquaMeasure DOT", "VR2AR"),
  "Serial#" = c("10755220", "670364", "547109"),
  "Depth" = c("2", "5", "15")
) %>% 
kable(align = "lcc")

```

### Title for this section (Raw data? Sensor export? Separate data files?)

The data from each sensor is exported to a separate *csv file, each with manufacturer-specific columns.

Import raw data files:
```{r, warning=FALSE}
path <- system.file("extdata", package = "strings")

hobo_raw <- read_csv(paste0(path, "/HOBO/10755220.csv"))

aquaMeasure_raw <- read_csv(paste0(path, "/aquaMeasure/aquaMeasure-670364_2019-10-19_UTC.csv"))

vemco_raw <-  read_csv(paste0(path, "/Vemco/Vemco_Borgles_Island_2019_05_30.csv"))

```

Raw Hobo data:
```{r}
head(hobo_raw)
```

Raw aquaMeasure data:
```{r}
head(aquaMeasure_raw)
```

Raw Vemco data:
```{r}
head(vemco_raw)
```


Something about how this data is messy / hard to work with in this format

### Compile data 

Compile data from the 3 sensors using `strings::compile_all_data()`:

```{r}

# deployment information for metadata:

# deployment and retrieval dates
deployment <- data.frame("START" = "2019-05-30", "END" = "2019-10-19")

serial.table.HOBO <- data.frame("SENSOR" = "HOBO-10755220", "DEPTH" = "2m")
serial.table.aM <- data.frame("SENSOR" = "aquaMeasure-670364", "DEPTH" = "5m")
depth.vemco <- "15m"


ALL_data <- compile_all_data(path = path,
                             deployment.range = deployment,
                             area.name = area,
                             # hobo
                             serial.table.HOBO = serial.table.HOBO,
                             # aquaMeasure
                             serial.table.aM = serial.table.aM,
                             # vemco
                             depth.vemco = depth.vemco)


```



The data is compiled in a "wide" format, with metadata in the first four rows indicating the deployment period, the sensor serial number, the variable and depth of the sensor, and the timezone of the timestamps. The first column is an index column, starting at -4 to account for the metadata rows. The remaining columns alternate between timestamp (in the format "Y-m-d H:M:S") and variable value (rounded to three decimal places). Note that because the sensors can be initialized at different times and record on different intervals, values in a single row do not necessarily correspond to the same timestamp. 













This format is convenient for human readers, who can quickly scan the metadata to determine the number of sensors deployed, the depths of deployment, etc. However, this format is less convenient for analysis (e.g., to include include the metadata, all values were converted to class character). Prior to analysis, the dataframe should be converted to a "long" ("tidy") format, and the values should be converted to the appropriate class (e.g., POSIXct for the timestamps and numeric for variable values). This can be done using the `convert_to_tidydata()` function, which returns a dataframe with the following columns:

* `DEPLOYMENT_RANGE`: The deployment and retrieval dates (character)
* `SENSOR`: The sensor that recorded the measurement (character)
* `TIMESTAMP`: The timestamp of the measurement (POSIXct)
* `VARIABLE`: The parameter measured (Temperature, Dissolved Oxygen, or Salinity) (character)
* `DEPTH`: The depth of the sensor (ordered factor)
* `VALUE:` The value of the measurement (numeric)
























